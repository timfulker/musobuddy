# API Usage Tracking - Implementation Summary

## ‚úÖ What's Been Completed

### 1. Database Schema
- ‚úÖ Added `api_usage_tracking` table (detailed logs)
- ‚úÖ Added `api_usage_stats` table (aggregated per-user stats)
- ‚úÖ Added `beta_email_templates` table
- ‚úÖ Migrations run in both dev and production Supabase

### 2. Backend Routes
- ‚úÖ `GET /api/admin/api-usage-stats` - Overall statistics
- ‚úÖ `GET /api/admin/api-usage-data` - Per-user detailed data
- ‚úÖ `GET /api/admin/api-costs` - Total API costs
- ‚úÖ `POST /api/admin/block-user-api` - Block users
- ‚úÖ `POST /api/admin/unblock-user-api` - Unblock users
- ‚úÖ `POST /api/admin/update-risk-score` - Update risk scores

### 3. Tracking Utility (`server/core/api-usage-tracker.ts`)
- ‚úÖ Automatic cost calculation for all AI models
- ‚úÖ Per-request tracking with metadata
- ‚úÖ Aggregated user statistics
- ‚úÖ Block/unblock functionality
- ‚úÖ Pricing database for OpenAI & Anthropic models

### 4. AI Orchestrator Integration
- ‚úÖ Added tracking to `server/services/ai-orchestrator.ts`
- ‚úÖ Tracks both OpenAI and Anthropic calls
- ‚úÖ Accepts optional `userId` parameter
- ‚úÖ Already integrated into booking message parser

## üìã What You Need to Do

### Required: Pass UserId Through Your Code

The tracking is now automatic in the AI orchestrator, but you need to pass `userId` parameter wherever AI calls are made:

#### Files That Need Updates:

1. **`server/core/ai-response-generator.ts` (Line 185)**
   ```typescript
   // BEFORE:
   const orchestrationResult = await aiOrchestrator.runTask('ai-response-generation', aiRequest, taskConfig);

   // AFTER (add userId parameter):
   const orchestrationResult = await aiOrchestrator.runTask('ai-response-generation', aiRequest, taskConfig, userId);
   ```
   - Find where this function gets `userId` from context
   - Pass it through to runTask

2. **`server/routes/support-chat-routes.ts` (Line 116)**
   ```typescript
   // BEFORE:
   const orchestrationResult = await aiOrchestrator.runTask('support-chat', aiRequest, taskConfig);

   // AFTER:
   const orchestrationResult = await aiOrchestrator.runTask('support-chat', aiRequest, taskConfig, req.user?.id);
   ```
   - This should have user from auth middleware

3. **`server/services/ai-event-matcher.ts` (Line 147)**
   ```typescript
   // BEFORE:
   const orchestrationResult = await aiOrchestrator.runTask('event-matching', aiRequest, taskConfig);

   // AFTER (add userId parameter to function signature and pass through):
   const orchestrationResult = await aiOrchestrator.runTask('event-matching', aiRequest, taskConfig, userId);
   ```
   - Add `userId?: string` to the function signature
   - Pass it from wherever this is called

4. **`server/services/booking-confirmation-parser.ts` (Line 113)**
   ```typescript
   // BEFORE:
   const orchestrationResult = await aiOrchestrator.runTask('confirmation-parsing', request, config);

   // AFTER:
   const orchestrationResult = await aiOrchestrator.runTask('confirmation-parsing', request, config, userId);
   ```
   - Add `userId?: string` to function signature
   - Pass from caller

### Optional: Add Direct OpenAI Tracking

If you have any direct OpenAI calls (not using the orchestrator), wrap them:

```typescript
import { trackApiUsage } from '../core/api-usage-tracker';

// After OpenAI call:
const completion = await openai.chat.completions.create({...});

// Add tracking:
await trackApiUsage({
  userId: user.id,
  service: 'openai',
  model: 'gpt-4o-mini',
  inputTokens: completion.usage?.prompt_tokens || 0,
  outputTokens: completion.usage?.completion_tokens || 0,
  metadata: { endpoint: 'chat.completions' }
});
```

## üîç How to Test

### 1. Check Admin Panel
1. Go to https://www.musobuddy.com/admin
2. Should see no errors now (200 responses, empty data)
3. API Usage section should show $0.00 and 0 requests initially

### 2. Trigger AI Usage
1. Send a test booking email (creates AI parsing call)
2. Generate an AI response in your app
3. Check admin panel again - should see:
   - Total requests incremented
   - Cost calculated and displayed
   - User breakdown showing your test user

### 3. Verify Database
```sql
-- Check tracking data
SELECT * FROM api_usage_tracking ORDER BY timestamp DESC LIMIT 10;

-- Check user stats
SELECT * FROM api_usage_stats;
```

## üìä What You'll See in Admin Panel

Once userId is passed through and users make AI requests:

- **Total Requests**: Count of all API calls
- **Total Cost**: Cumulative cost in USD
- **Per-User Breakdown**:
  - User name and email
  - Total requests
  - Total cost
  - Last activity timestamp
  - Service breakdown (OpenAI vs Anthropic)
  - Risk score
  - Block status

## üí∞ Pricing Reference

The tracker uses these rates (per 1M tokens):

### OpenAI
- GPT-4o: $2.50 input / $10.00 output
- GPT-4o-mini: $0.15 input / $0.20 output
- GPT-3.5-turbo: $0.50 input / $1.50 output

### Anthropic
- Claude Sonnet 4: $3.00 input / $15.00 output
- Claude Opus: $15.00 input / $75.00 output
- Claude Haiku: $0.25 input / $1.25 output

## üöÄ Deployment Checklist

- [x] Database migrations run
- [x] API routes deployed
- [x] Tracking utility created
- [x] AI orchestrator updated
- [ ] Update 4 files to pass userId (see above)
- [ ] Deploy changes
- [ ] Test with actual AI call
- [ ] Verify admin panel shows data

## üõ†Ô∏è Maintenance

### Block a User
```typescript
POST /api/admin/block-user-api
{ "userId": "user-id", "reason": "Excessive usage" }
```

### Update Risk Score
```typescript
POST /api/admin/update-risk-score
{ "userId": "user-id", "riskScore": 75 }
```

### Check Costs Per Service
The `serviceBreakdown` JSON field shows:
```json
{
  "openai": { "requests": 150, "cost": 2.50, "tokens": 50000 },
  "anthropic": { "requests": 25, "cost": 0.75, "tokens": 15000 }
}
```

## ‚ùì FAQ

**Q: Why is the admin panel showing empty data?**
A: The userId isn't being passed through yet. Update the 4 files listed above.

**Q: How accurate are the costs?**
A: Very accurate - uses official pricing and actual token counts from API responses.

**Q: Can I track other APIs (Google, etc.)?**
A: Yes! Use `trackApiUsage()` with `service: 'google'` or `service: 'other'`.

**Q: Will this slow down my API calls?**
A: No - tracking happens async and doesn't block the response. Errors in tracking don't affect the main flow.

**Q: What if a user is blocked?**
A: They'll get an error: "API access blocked for this user" before the AI call is made.
